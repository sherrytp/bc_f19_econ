{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution (Naive) Multiple Custom Grouping Aggregations\n",
    "\n",
    "This challenge is going to be fairly difficult, but should answer a question that many pandas users face - What is the best way to do a grouping operation that does many custom aggregations? In this context, a 'custom aggregation' is defined as one that is not directly available to use from pandas and one that you must write a custom function for. \n",
    "\n",
    "In Pandas Challenge 1, a single aggregation, which required a custom grouping function, was the desired result. In this challenge, you'll need to make several aggregations when grouping. There are a few different solutions to this problem, but depending on how you arrive at your solution, there could arise enormous performance differences. I am looking for a compact, readable solution with very good performance.\n",
    "\n",
    "### Sales Data\n",
    "\n",
    "In this challenge, you will be working with some mock sales data found in the sales.csv file. It contains 200,000 rows and 9 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>delivery_type</th>\n",
       "      <th>cost_type</th>\n",
       "      <th>duration</th>\n",
       "      <th>revenue</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>13763</td>\n",
       "      <td>2019-03-25</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>F</td>\n",
       "      <td>slow</td>\n",
       "      <td>expert</td>\n",
       "      <td>60</td>\n",
       "      <td>553</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13673</td>\n",
       "      <td>2019-12-06</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>I</td>\n",
       "      <td>slow</td>\n",
       "      <td>experienced</td>\n",
       "      <td>60</td>\n",
       "      <td>895</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10287</td>\n",
       "      <td>2018-09-04</td>\n",
       "      <td>India</td>\n",
       "      <td>I</td>\n",
       "      <td>slow</td>\n",
       "      <td>novice</td>\n",
       "      <td>60</td>\n",
       "      <td>857</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>14298</td>\n",
       "      <td>2018-06-21</td>\n",
       "      <td>Morocco</td>\n",
       "      <td>F</td>\n",
       "      <td>fastest</td>\n",
       "      <td>expert</td>\n",
       "      <td>120</td>\n",
       "      <td>741</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11523</td>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>A</td>\n",
       "      <td>fast</td>\n",
       "      <td>expert</td>\n",
       "      <td>120</td>\n",
       "      <td>942</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id       date     country region delivery_type    cost_type  \\\n",
       "0        13763 2019-03-25    Portugal      F          slow       expert   \n",
       "1        13673 2019-12-06   Singapore      I          slow  experienced   \n",
       "2        10287 2018-09-04       India      I          slow       novice   \n",
       "3        14298 2018-06-21     Morocco      F       fastest       expert   \n",
       "4        11523 2019-01-05  Luxembourg      A          fast       expert   \n",
       "\n",
       "   duration  revenue  cost  \n",
       "0        60      553   295  \n",
       "1        60      895   262  \n",
       "2        60      857   260  \n",
       "3       120      741   238  \n",
       "4       120      942   263  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/sales.csv', parse_dates=['date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "There are many aggregations that you will need to return and it will take some time to understand what they are and how to return them. The following definitions for two time periods will be used throughout the aggregations.\n",
    "\n",
    "Period **2019H1** is defined as the time period beginning January 1, 2019 and ending June 30, 2019.\n",
    "Period **2018H1** is defined as the time period beginning January 1, 2018 and ending June 30, 2018.\n",
    "\n",
    "### Aggregations\n",
    "Now, I will list all the aggregations that are expected to be returned. Each bullet point represents a single column. Use the first word after the bullet point as the new column name.\n",
    "\n",
    "For every country and region, return the following:\n",
    "* recency: Number of days between today's date (9/9/2019) and the maximum value of the 'date' column \n",
    "* fast_and_fastest: Number of unique customer_id in period 2019H1 with delivery_type either 'fast' or 'fastest'\n",
    "* rev_2019: Total revenue for the period 2019H1\n",
    "* rev_2018: Total revenue for the period 2018H1\n",
    "* cost_2019: Total cost for period 2019H1\n",
    "* cost_2019_exp: Total cost for period 2019H1 with cost_type 'expert'\n",
    "* other_cost: Difference between cost_2019 and cost_2019_exp\n",
    "* rev_per_60: Total of revenue when duration equals 60 in period 2019H1 divided by number of unique customer_id when duration equals 60 in period 2019H1 \n",
    "* profit_margin: Take the difference of rev_2019 and cost_2019_exp then divide by rev_2019. Return as percentage\n",
    "* cost_exp_per_60: Total of cost when duration is 60 and cost_type is 'expert' in period 2019H1 divided by the number of unique customer_id when duration equals 60 and cost_type is 'expert' in period 2019H1 \n",
    "* growth: Find the percentage growth from revenue in period 2019H1 compared to the revenue in period 2018H1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "I will first present a naive solution that returns the correct results, but is extremely slow. It uses a large custom function with the groupby `apply` method. Using the groupby `apply` method has potential to capsize your program as performance can be awful. \n",
    "\n",
    "One of my first attempts at using a groupby `apply` to solve a complex grouping problem resulted in a computation that took about eight hours to finish. The dataset was fairly large, at around a million rows, but could still easily fit in memory. I eventually ended up solving the problem using SAS (and not pandas) and shrank the execution time down to a few minutes. This is not an endorsement of SAS, but rather a warning that poor knowledge of how pandas works can lead to horrific performance issues.\n",
    "\n",
    "### Built-in vs Custom groupby functions\n",
    "\n",
    "This is a difficult challenge because each aggregation requires a custom calculation that is not provided directly as a pandas groupby method. For example, taking the sum of a column in each group is a simple groupby operation that is built into pandas. No custom function is needed. Here is an example of how we can sum all the revenue for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"5\" valign=\"top\">Argentina</td>\n",
       "      <td>A</td>\n",
       "      <td>284805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>B</td>\n",
       "      <td>293080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>C</td>\n",
       "      <td>277825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>D</td>\n",
       "      <td>264302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>E</td>\n",
       "      <td>306655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  revenue\n",
       "country   region         \n",
       "Argentina A        284805\n",
       "          B        293080\n",
       "          C        277825\n",
       "          D        264302\n",
       "          E        306655"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['country', 'region']).agg({'revenue':'sum'}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the performance of this operation with the timeit magic command yields about 25ms for completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.4 ms ± 968 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df.groupby(['country', 'region']).agg({'revenue':'sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can instead define a very simple custom function which computes the same sum. A pandas Series of the revenue values is passed to the custom function for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sum(x):\n",
    "    return x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"5\" valign=\"top\">Argentina</td>\n",
       "      <td>A</td>\n",
       "      <td>284805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>B</td>\n",
       "      <td>293080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>C</td>\n",
       "      <td>277825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>D</td>\n",
       "      <td>264302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>E</td>\n",
       "      <td>306655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  revenue\n",
       "country   region         \n",
       "Argentina A        284805\n",
       "          B        293080\n",
       "          C        277825\n",
       "          D        264302\n",
       "          E        306655"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['country', 'region']).agg({'revenue': custom_sum}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the performance of this custom function, which only has a single line of code has already more than doubled execution time to 60ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.7 ms ± 4.79 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df.groupby(['country', 'region']).agg({'revenue': custom_sum})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define a slightly more demanding custom function, one where we return the sum of the revenue based on whether the cost_type column is 'expert'. Here, we must use `apply` as it passes the entire DataFrame of each group to the custom function and not just the Series like `agg` does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sum2(x):\n",
    "    is_exp = x['cost_type'] == 'expert'\n",
    "    return x.loc[is_exp, 'revenue'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country    region\n",
       "Argentina  A          99605\n",
       "           B          93364\n",
       "           C          87975\n",
       "           D          89515\n",
       "           E         110613\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['country', 'region']).apply(custom_sum2).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure performance again (350ms) and see now that we are more than an order of magnitude worse than the original and the only difference is a simple filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439 ms ± 56.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df.groupby(['country', 'region']).apply(custom_sum2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a better way to solve this problem and will be shown in the optimal second solution. Let's first complete our challenge using this naive approach with a custom function passed to the `apply` groupby method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Solution - Use custom function with apply\n",
    "\n",
    "This challenge lists 11 columns that must be returned in the result. Each of the columns returned requires some kind of customized operation that is not built into the pandas groupby. Let's begin by calculating the first column, recency. \n",
    "\n",
    "For every combination of country and region, we need to calculate the number of days between today and the 'maximum' (most recent) date. To do this we write the custom function, `f1`, which will accept the entire DataFrame as its argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    today = pd.Timestamp('today')\n",
    "    most_recent = x['date'].max()\n",
    "    recency = (today - most_recent).days\n",
    "    return recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country    region\n",
       "Argentina  A        -83\n",
       "           B        -85\n",
       "           C        -85\n",
       "           D        -85\n",
       "           E        -85\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['country', 'region']).apply(f1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of returning the scalar result, we can return a Series with the index equal to the column name that we'd like to use, which will be the format for the remainder of this solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    today = pd.Timestamp('today')\n",
    "    most_recent = x['date'].max()\n",
    "    recency = (today - most_recent).days\n",
    "    d = {\n",
    "        'recency': recency\n",
    "        }\n",
    "    return pd.Series(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this function to each group yields the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>recency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"5\" valign=\"top\">Argentina</td>\n",
       "      <td>A</td>\n",
       "      <td>-83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>B</td>\n",
       "      <td>-85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>C</td>\n",
       "      <td>-85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>D</td>\n",
       "      <td>-85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>E</td>\n",
       "      <td>-85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  recency\n",
       "country   region         \n",
       "Argentina A           -83\n",
       "          B           -85\n",
       "          C           -85\n",
       "          D           -85\n",
       "          E           -85"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['country', 'region']).apply(f1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure performance and get 350ms. This doesn't seem too bad for a dataset of 200k rows, but we have 10 more columns to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358 ms ± 45.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df.groupby(['country', 'region']).apply(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to the second column, rev_2019. We need to create a filter for revenue for only the first half of the year 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    is_2019H1 = x['date'].between('2019-01-01', '2019-06-30')\n",
    "    recency = (pd.Timestamp('today') - x['date'].max()).days\n",
    "    rev_2019 =         x.loc[is_2019H1, 'revenue'].sum()\n",
    "    \n",
    "    d = {\n",
    "        'recency': recency,\n",
    "        'rev_2019': rev_2019\n",
    "        }\n",
    "    return pd.Series(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this new function to our groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>recency</th>\n",
       "      <th>rev_2019</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"5\" valign=\"top\">Argentina</td>\n",
       "      <td>A</td>\n",
       "      <td>-83</td>\n",
       "      <td>150508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>B</td>\n",
       "      <td>-85</td>\n",
       "      <td>139048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>C</td>\n",
       "      <td>-85</td>\n",
       "      <td>118035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>D</td>\n",
       "      <td>-85</td>\n",
       "      <td>131728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>E</td>\n",
       "      <td>-85</td>\n",
       "      <td>146201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  recency  rev_2019\n",
       "country   region                   \n",
       "Argentina A           -83    150508\n",
       "          B           -85    139048\n",
       "          C           -85    118035\n",
       "          D           -85    131728\n",
       "          E           -85    146201"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['country', 'region']).apply(f2).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring performance again and we've jumped up to nearly 1 full second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "915 ms ± 26.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df.groupby(['country', 'region']).apply(f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue to develop our custom function until all columns are calculated in this manner. Here is the final function with all the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_final(x):\n",
    "    # filters\n",
    "    is_2019H1 =        x['date'].between('2019-01-01', '2019-06-30')\n",
    "    is_2018H1 =        x['date'].between('2018-01-01', '2018-06-30')\n",
    "    is_fast_fastest =  x['delivery_type'].isin({'fast', 'fastest'})\n",
    "    is_exp =           x['cost_type'] == 'expert'\n",
    "    is_60 =            x['duration'] == 60\n",
    "    is_2019H1_exp =    is_2019H1 & is_exp\n",
    "    is_2019H1_60 =     is_2019H1 & is_60\n",
    "    is_2019H1_60_exp = is_2019H1 & is_60 & is_exp\n",
    "    \n",
    "    # column calculations\n",
    "    recency =          (pd.Timestamp('today') - x['date'].max()).days\n",
    "    fast_and_fastest = x.loc[is_fast_fastest, 'customer_id'].nunique()\n",
    "    rev_2019 =         x.loc[is_2019H1, 'revenue'].sum()\n",
    "    rev_2018 =         x.loc[is_2018H1, 'revenue'].sum()\n",
    "    cost_2019 =        x.loc[is_2019H1, 'cost'].sum()\n",
    "    cost_2019_exp =    x.loc[is_2019H1_exp, 'cost'].sum()\n",
    "        \n",
    "    # helper calculations\n",
    "    rev_2019_60 =           x.loc[is_2019H1_60, 'revenue'].sum()\n",
    "    uniq_cust_2019_60 =     x.loc[is_2019H1_60, 'customer_id'].nunique()\n",
    "    cost_2019_exp_60 =      x.loc[is_2019H1_60_exp, 'cost'].sum()\n",
    "    uniq_cust_2019_exp_60 = x.loc[is_2019H1_60_exp, 'customer_id'].nunique()\n",
    "    \n",
    "    # more column calculations\n",
    "    other_cost =       cost_2019 - cost_2019_exp\n",
    "    rev_per_60 =       rev_2019_60 / uniq_cust_2019_60\n",
    "    profit_margin =    (rev_2019 - cost_2019_exp) / rev_2019 * 100\n",
    "    cost_exp_per_60 =  cost_2019_exp_60 / uniq_cust_2019_60\n",
    "    growth =           (rev_2019 - rev_2018) / rev_2018 * 100\n",
    "    \n",
    "    d = {\n",
    "        'recency': recency,\n",
    "        'fast_and_fastest': fast_and_fastest,\n",
    "        'rev_2019': rev_2019,\n",
    "        'rev_2018': rev_2018,\n",
    "        'cost_2019': cost_2019,\n",
    "        'cost_2019_exp': cost_2019_exp,\n",
    "        'other_cost': other_cost,\n",
    "        'rev_per_60': rev_per_60,\n",
    "        'profit_margin': profit_margin,\n",
    "        'cost_exp_per_60': cost_exp_per_60,\n",
    "        'growth': growth\n",
    "    }\n",
    "    \n",
    "    return pd.Series(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this final function and formatting the resulting DataFrame yields the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68c\" ><thead>    <tr>        <th class=\"blank\" ></th>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >recency</th>        <th class=\"col_heading level0 col1\" >fast_and_fastest</th>        <th class=\"col_heading level0 col2\" >rev_2019</th>        <th class=\"col_heading level0 col3\" >rev_2018</th>        <th class=\"col_heading level0 col4\" >cost_2019</th>        <th class=\"col_heading level0 col5\" >cost_2019_exp</th>        <th class=\"col_heading level0 col6\" >other_cost</th>        <th class=\"col_heading level0 col7\" >rev_per_60</th>        <th class=\"col_heading level0 col8\" >profit_margin</th>        <th class=\"col_heading level0 col9\" >cost_exp_per_60</th>        <th class=\"col_heading level0 col10\" >growth</th>    </tr>    <tr>        <th class=\"index_name level0\" >country</th>        <th class=\"index_name level1\" >region</th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>        <th class=\"blank\" ></th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68clevel0_row0\" class=\"row_heading level0 row0\" rowspan=5>Argentina</th>\n",
       "                        <th id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68clevel1_row0\" class=\"row_heading level1 row0\" >A</th>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow0_col0\" class=\"data row0 col0\" >-83</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow0_col1\" class=\"data row0 col1\" >138</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow0_col2\" class=\"data row0 col2\" >150,508</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow0_col3\" class=\"data row0 col3\" >82,912</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow0_col4\" class=\"data row0 col4\" >49,577</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow0_col5\" class=\"data row0 col5\" >18,553</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow0_col6\" class=\"data row0 col6\" >31,024</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow0_col7\" class=\"data row0 col7\" >773</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow0_col8\" class=\"data row0 col8\" >88</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow0_col9\" class=\"data row0 col9\" >111</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow0_col10\" class=\"data row0 col10\" >82</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68clevel1_row1\" class=\"row_heading level1 row1\" >B</th>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow1_col0\" class=\"data row1 col0\" >-85</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow1_col1\" class=\"data row1 col1\" >143</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow1_col2\" class=\"data row1 col2\" >139,048</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow1_col3\" class=\"data row1 col3\" >92,112</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow1_col4\" class=\"data row1 col4\" >46,153</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow1_col5\" class=\"data row1 col5\" >15,732</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow1_col6\" class=\"data row1 col6\" >30,421</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow1_col7\" class=\"data row1 col7\" >750</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow1_col8\" class=\"data row1 col8\" >89</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow1_col9\" class=\"data row1 col9\" >97</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow1_col10\" class=\"data row1 col10\" >51</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68clevel1_row2\" class=\"row_heading level1 row2\" >C</th>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow2_col0\" class=\"data row2 col0\" >-85</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow2_col1\" class=\"data row2 col1\" >129</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow2_col2\" class=\"data row2 col2\" >118,035</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow2_col3\" class=\"data row2 col3\" >98,472</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow2_col4\" class=\"data row2 col4\" >38,786</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow2_col5\" class=\"data row2 col5\" >12,661</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow2_col6\" class=\"data row2 col6\" >26,125</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow2_col7\" class=\"data row2 col7\" >780</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow2_col8\" class=\"data row2 col8\" >89</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow2_col9\" class=\"data row2 col9\" >105</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow2_col10\" class=\"data row2 col10\" >20</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68clevel1_row3\" class=\"row_heading level1 row3\" >D</th>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow3_col0\" class=\"data row3 col0\" >-85</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow3_col1\" class=\"data row3 col1\" >135</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow3_col2\" class=\"data row3 col2\" >131,728</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow3_col3\" class=\"data row3 col3\" >79,600</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow3_col4\" class=\"data row3 col4\" >44,190</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow3_col5\" class=\"data row3 col5\" >17,217</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow3_col6\" class=\"data row3 col6\" >26,973</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow3_col7\" class=\"data row3 col7\" >730</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow3_col8\" class=\"data row3 col8\" >87</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow3_col9\" class=\"data row3 col9\" >88</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow3_col10\" class=\"data row3 col10\" >65</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <th id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68clevel1_row4\" class=\"row_heading level1 row4\" >E</th>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow4_col0\" class=\"data row4 col0\" >-85</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow4_col1\" class=\"data row4 col1\" >177</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow4_col2\" class=\"data row4 col2\" >146,201</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow4_col3\" class=\"data row4 col3\" >93,119</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow4_col4\" class=\"data row4 col4\" >49,600</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow4_col5\" class=\"data row4 col5\" >18,372</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow4_col6\" class=\"data row4 col6\" >31,228</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow4_col7\" class=\"data row4 col7\" >747</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow4_col8\" class=\"data row4 col8\" >87</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow4_col9\" class=\"data row4 col9\" >110</td>\n",
       "                        <td id=\"T_3b0ddbaa_d598_11e9_a05c_b8e85647e68crow4_col10\" class=\"data row4 col10\" >57</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x119c4d390>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.groupby(['country', 'region']).apply(f_final)\n",
    "df1.head().style.format('{:,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our performance with this final function is more than 3.5 seconds. While this is a lot less than 8 hours, the calculations we performed in the custom function were fairly simple and our data was just 200k rows. If the data and complexity of the custom function increases by an 1-2 order of magnitudes each, hours of computation time await."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.68 s ± 78.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit df.groupby(['country', 'region']).apply(f_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Become a pandas expert\n",
    "\n",
    "If you are looking to completely master the pandas library and become a trusted expert for doing data science work, check out my book [Master Data Analysis with Python][1]. It comes with over 300 exercises with detailed solutions covering the pandas library in-depth.\n",
    "\n",
    "[1]: https://www.dunderdata.com/master-data-analysis-with-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
