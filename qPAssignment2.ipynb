{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(tar, fname):\n",
    "    member = tar.getmember(fname)\n",
    "    print(member.name)\n",
    "    tf = tar.extractfile(member)\n",
    "    data = []\n",
    "    labels = []\n",
    "    for line in tf:\n",
    "        line = line.decode(\"utf-8\")\n",
    "        (label,text) = line.strip().split(\"\\t\")\n",
    "        labels.append(label)\n",
    "        data.append(text)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(tarfname):\n",
    "    import tarfile\n",
    "    tar = tarfile.open(tarfname, \"r:gz\")\n",
    "    \n",
    "    trainname = \"train.tsv\"\n",
    "    devname = \"dev.tsv\"\n",
    "    unlabeledname = \"unlabeled.tsv\"\n",
    "    for member in tar.getmembers():\n",
    "        if 'train.tsv' in member.name:\n",
    "            trainname = member.name\n",
    "        elif 'dev.tsv' in member.name:\n",
    "            devname = member.name\n",
    "        elif 'unlabeled.tsv' in member.name:\n",
    "            unlabeledname = member.name\n",
    "\n",
    "    # no sentiment? from sentiment import read_tsv            \n",
    "    class Data: pass\n",
    "    sentiment = Data()\n",
    "    sentiment.train_data, sentiment.train_labels = read_tsv(tar,trainname)\n",
    "    print(f\"train data num: {len(sentiment.train_data)}\")\n",
    "    sentiment.dev_data, sentiment.dev_labels = read_tsv(tar, devname)\n",
    "    print(f\"dev data num: {len(sentiment.dev_data)}\")\n",
    "    sentiment.test_data = []\n",
    "    tf = tar.extractfile(unlabeledname)\n",
    "    for line in tf:\n",
    "        line = line.decode(\"utf-8\")\n",
    "        text = line.strip()\n",
    "        sentiment.test_data.append(text)\n",
    "    print(f\"test data num: {len(sentiment.test_data)}\")\n",
    "\n",
    "    tar.close()\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyize(sentiment):\n",
    "    import nltk\n",
    "    sentiment.train_data_divided = [[], []]\n",
    "    for index, label in enumerate(sentiment.train_labels):\n",
    "        sentiment.train_data_divided[int(label == 'NEGATIVE')].append(sentiment.train_data[index])\n",
    "    \n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english')) \n",
    "    tags = [\"JJ\", \"JJR\", \"JJS\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "    vocabularies = [{}, {}]\n",
    "    for index, data in enumerate(sentiment.train_data_divided):\n",
    "        for document in data:\n",
    "            token_tags = nltk.pos_tag(tokenizer.tokenize(document))\n",
    "            for token, tag in token_tags:\n",
    "                if token not in stopwords and tag in tags and len(token) > 1:\n",
    "                    if token in vocabularies[index]:\n",
    "                        vocabularies[index][token] += 1\n",
    "                    else:\n",
    "                        vocabularies[index][token] = 1\n",
    "    \n",
    "    print(sorted(vocabularies[0].items(), key=lambda x: x[1], reverse=True))\n",
    "    print()\n",
    "    print(sorted(vocabularies[1].items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def transformX(sentiment, ngram_range=(1,4), min_df=2):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "    from nltk import word_tokenize\n",
    "    sentiment.count_vect = CountVectorizer(tokenizer=word_tokenize, ngram_range=ngram_range, min_df=min_df)\n",
    "    \n",
    "    sentiment.trainX = sentiment.count_vect.fit_transform(sentiment.train_data)\n",
    "    sentiment.devX = sentiment.count_vect.transform(sentiment.dev_data)\n",
    "    sentiment.testX = sentiment.count_vect.transform(sentiment.test_data)\n",
    "    print(f\"feature num: {sentiment.trainX.shape[1]}\")\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "def transformY(sentiment):\n",
    "    from sklearn import preprocessing\n",
    "    sentiment.le = preprocessing.LabelEncoder()\n",
    "    \n",
    "    sentiment.le.fit(sentiment.train_labels)\n",
    "    sentiment.target_labels = sentiment.le.classes_\n",
    "    sentiment.trainy = sentiment.le.transform(sentiment.train_labels)\n",
    "    sentiment.devy = sentiment.le.transform(sentiment.dev_labels)\n",
    "    \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X, y):\n",
    "    \"\"\"Train a classifier using the given training data.\n",
    "\n",
    "    Trains logistic regression on the input data with default parameters.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    cls = LogisticRegression(C=0.8, random_state=0, solver='sag', max_iter=10000)\n",
    "    cls.fit(X, y)\n",
    "    return cls\n",
    "\n",
    "def evaluate(X, yt, cls, name='data'):\n",
    "    \"\"\"Evaluated a classifier on the given labeled data using accuracy.\"\"\"\n",
    "    from sklearn import metrics\n",
    "    yp = cls.predict(X)\n",
    "    acc = metrics.accuracy_score(yt, yp)\n",
    "    print(\"Accuracy on %s  is: %s\" % (name, acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Reading data\")\n",
    "# tarfname = \"data/sentiment.tar.gz\"\n",
    "# sentiment = read_files(tarfname)\n",
    "# sentiment = transformY(sentiment)\n",
    "# print()\n",
    "\n",
    "# for max_ngram in range(2, 6):\n",
    "#     print(f'max_ngram: {max_ngram}')\n",
    "#     for min_df in range(1, 4):\n",
    "#         print(f'min_df: {min_df}')\n",
    "#         sentiment = transformX(sentiment, ngram_range=(1, max_ngram), min_df=min_df)\n",
    "        \n",
    "#         cls = train_classifier(sentiment.trainX, sentiment.trainy)\n",
    "#         evaluate(sentiment.trainX, sentiment.trainy, cls, 'train')\n",
    "#         evaluate(sentiment.devX, sentiment.devy, cls, 'dev')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'/kaggle/working\\n'\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run(['pwd'], shell=True)\n",
    "print(subprocess.Popen(\"pwd\", shell=True, stdout=subprocess.PIPE).stdout.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '../input/sentiment/sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b52bfe19c19f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtarfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../input/sentiment/sentiment\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#tarfname = \"../input/sentiment-pred.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ef4c85cd6d47>\u001b[0m in \u001b[0;36mread_files\u001b[0;34m(tarfname)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r:gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrainname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train.tsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1585\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCompressionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown compression type %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcomptype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1587\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1589\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"|\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/tarfile.py\u001b[0m in \u001b[0;36mgzopen\u001b[0;34m(cls, name, mode, fileobj, compresslevel, **kwargs)\u001b[0m\n\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1635\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '../input/sentiment/sentiment'"
     ]
    }
   ],
   "source": [
    "tarfname = \"../input/sentiment/sentiment\"\n",
    "#tarfname = \"../input/sentiment-pred.csv\"\n",
    "sentiment = read_files(tarfname)\n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "os.path.isfile(tarfname)\n",
    "os.listdir(tarfname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-9-735e1aa5e2eb>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-735e1aa5e2eb>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    if accuracy >= max_accuracy:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "sentiment = transformX(sentiment)\n",
    "sentiment = transformY(sentiment)\n",
    "    \n",
    "max_accuracy = 0.0\n",
    "while True:\n",
    "    cls = train_classifier(sentiment.trainX, sentiment.trainy)\n",
    "    \n",
    "    # Stop criterion\n",
    "    accuracy = evaluate(sentiment.devX, sentiment.devy, cls, 'dev')\n",
    "     if accuracy >= max_accuracy:\n",
    "         max_accuracy = accuracy\n",
    "     else:\n",
    "         break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fc777e358672>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Expand the training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentiment' is not defined"
     ]
    }
   ],
   "source": [
    "# Expand the training set\n",
    "labels = sentiment.le.inverse_transform(cls.predict(sentiment.testX))\n",
    "scores = cls.decision_function(sentiment.testX)\n",
    "for index, score in enumerate(scores):\n",
    "    if score > 5 or score < -5:\n",
    "        sentiment.train_data.append(sentiment.test_data[index])\n",
    "        sentiment.train_labels.append(labels[index])\n",
    "sentiment.trainX = sentiment.count_vect.transform(sentiment.train_data)\n",
    "sentiment.trainy = sentiment.le.transform(sentiment.train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#    print(\"Reading data\")\n",
    "#    tarfname = \"../input/sentiment/sentiment\"\n",
    "#    sentiment = read_files(tarfname)\n",
    "#    print(\"\\nTraining classifier\")\n",
    "#    while True: \n",
    "#        import classify\n",
    "#        cls = classify.train_classifier(sentiment.trainX, sentiment.trainy)\n",
    "#    print(\"\\nEvaluating\")\n",
    "#    classify.evaluate(sentiment.trainX, sentiment.trainy, cls, 'train')\n",
    "#    classify.evaluate(sentiment.devX, sentiment.devy, cls, 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"{python}\\nfrom subprocess import check_output\\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\\n\\n# Any results you write to the current directory are saved as output.\\nkey_1.csv\\nsample_submission_1.csv\\ntrain_1.csv\\n\\nsample_sub = \"../input/sample_submission_1.csv\"\\n\\ndata = pd.read_csv(sample_sub)\\ndata[\\'Visits\\'] = data[\\'Visits\\'] + 32\\ndata.to_csv(\"32.csv\", index=False)\\n```\\n\\n\\n``` \\nciti <- income[1:68,]\\nmacroeco <- merge(macroeco, citi[, c(\"date\", \"provision.losses\")], by = \"date\", \\n                  all.x = TRUE, all.y = TRUE)\\npairs(~provision.losses + real.gdp.growth + nominal.gdp.growth + real.disposable.income.growth + \\n        nominal.disposable.income.growth + unemployment.rate + cpi.inflation.rate + three + five \\n      + ten + bbb.corporate.yield + mortgage.rate + prime.rate\\n      + dow.jones.total.stock.market.index.level + house.price.index.level\\n      + commercial.real.estate.price.index.level + market.volatility.index.level\\n      , data = macroeco, main = \"Citigroup Loss Scatterplot\")\\ncor(macroeco[, -c(1,17)], use = \"complete.obs\")\\n```\\n\\nRun Time-Series Regression \\n``` {r}\\nmacroeco$date <- ts(macroeco$date)\\ncitireg <- tslm(provision.losses ~ real.gdp.growth + unemployment.rate + three\\n             + dow.jones.total.stock.market.index.level + prime.rate, data = macroeco)\\nsummary(citireg)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"{python}\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "key_1.csv\n",
    "sample_submission_1.csv\n",
    "train_1.csv\n",
    "\n",
    "sample_sub = \"../input/sample_submission_1.csv\"\n",
    "\n",
    "data = pd.read_csv(sample_sub)\n",
    "data['Visits'] = data['Visits'] + 32\n",
    "data.to_csv(\"32.csv\", index=False)\n",
    "```\n",
    "\n",
    "\n",
    "``` \n",
    "citi <- income[1:68,]\n",
    "macroeco <- merge(macroeco, citi[, c(\"date\", \"provision.losses\")], by = \"date\", \n",
    "                  all.x = TRUE, all.y = TRUE)\n",
    "pairs(~provision.losses + real.gdp.growth + nominal.gdp.growth + real.disposable.income.growth + \n",
    "        nominal.disposable.income.growth + unemployment.rate + cpi.inflation.rate + three + five \n",
    "      + ten + bbb.corporate.yield + mortgage.rate + prime.rate\n",
    "      + dow.jones.total.stock.market.index.level + house.price.index.level\n",
    "      + commercial.real.estate.price.index.level + market.volatility.index.level\n",
    "      , data = macroeco, main = \"Citigroup Loss Scatterplot\")\n",
    "cor(macroeco[, -c(1,17)], use = \"complete.obs\")\n",
    "```\n",
    "\n",
    "Run Time-Series Regression \n",
    "``` {r}\n",
    "macroeco$date <- ts(macroeco$date)\n",
    "citireg <- tslm(provision.losses ~ real.gdp.growth + unemployment.rate + three\n",
    "             + dow.jones.total.stock.market.index.level + prime.rate, data = macroeco)\n",
    "summary(citireg)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
